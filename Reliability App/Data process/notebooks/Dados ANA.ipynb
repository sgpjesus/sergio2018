{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import sys \n",
    "sys.path.append(r'C:\\Users\\sergiojesus\\Desktop\\Diogo2017\\EQS_LOCAL\\Bitbucket_eqs\\analytics_stat')\n",
    "import distributions as di\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import datetime\n",
    "import scipy.stats as s\n",
    "#import scipy.optimize as opt\n",
    "#import scipy.special as special\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import qgrid\n",
    "from IPython.display import HTML, Image, display, clear_output\n",
    "# display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# py.sign_in('diogo.antunes.goncalves', 'kt7T5JadNn2AhWk3DdPR')\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "import traitlets\n",
    "from tkinter import Tk, filedialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.language import Language\n",
    "import spacy\n",
    "import re\n",
    "import unidecode\n",
    "import edit_distance_nltk\n",
    "\n",
    "nlp = Language(Vocab()) #Spacy variables\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "def tokenization_process(string):\n",
    "    \"\"\"tokenizes one string, removing not used punctuation in portuguese language in the process\"\"\"\n",
    "    aux = tokenizer(string)\n",
    "    output = list()\n",
    "    for index, word in enumerate(aux):\n",
    "        if re.search('([A-Za-zÀ-ÿ]+(-|)[A-Za-zÀ-ÿ]+|[A-Za-zÀ-ÿ]+)', str(word)):\n",
    "                output.append(str(re.search('([A-Za-zÀ-ÿ]+(-|)[A-Za-zÀ-ÿ]+|[A-Za-zÀ-ÿ]+)', str(word)).group(0)).lower())\n",
    "    return output\n",
    "\n",
    "def toptokens(Tfidfmatrix, tokens, n=20):\n",
    "    npmatrix = Tfidfmatrix.toarray()\n",
    "    npmatrix[npmatrix == 0] = np.nan\n",
    "    npmeans = np.nanmean(npmatrix, axis = 0)\n",
    "    ordered_ids = np.argsort(npmeans)[::-1] \n",
    "    clean_ids = ordered_ids[(np.isnan(npmeans).sum()):]\n",
    "    top_ids = clean_ids[:n]\n",
    "    toptoken = [(tokens[i], npmeans[i]) for i in top_ids]\n",
    "    df = pd.DataFrame(toptoken)\n",
    "    df.columns = ['feature', 'TF-IDF']\n",
    "    return df\n",
    "\n",
    "def join_series(series_list):\n",
    "    num_elements = series_list[0].shape[0]\n",
    "    string_list = list()\n",
    "    for i in range(num_elements):\n",
    "        new_string = str()\n",
    "        for column, value in enumerate(series_list):\n",
    "            new_string = new_string + unidecode.unidecode(series_list[column][i]) + ' '\n",
    "        string_list.append(new_string)\n",
    "    new_series = pd.Series(data = string_list, index = range(num_elements))\n",
    "    return new_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Display:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Select file button\n",
    "        self._bsf = widgets.Button(layout=widgets.Layout(width='500px',height='50px'))\n",
    "        self._bsf.add_traits(files=traitlets.traitlets.List())\n",
    "        self._bsf.description = \"Select File\"\n",
    "        self._bsf.icon = \"square-o\"\n",
    "        self._bsf.on_click(self.select_excel)\n",
    "        \n",
    "        \n",
    "        # Load Excel button\n",
    "        self._blj = widgets.Button(layout=widgets.Layout(width='500px',height='50px'))\n",
    "        self._blj.description = \"Load File\"\n",
    "        self._blj.icon = 'download'\n",
    "        self._blj.on_click(self.load_excel)\n",
    "        self._blj.disabled = True\n",
    "                \n",
    "        # Spreadsheet Dropdown\n",
    "        style = {'description_width': 'initial'}\n",
    "        self._dds = widgets.Dropdown(options=['Select file'], value='Select file', disabled=True, style=style,\n",
    "                                     description='Spreadsheet:')\n",
    "        \n",
    "        # Column Selection\n",
    "        self._csm = widgets.SelectMultiple(options=['Select table'], value =['Select table'], disabled=True)\n",
    "        \n",
    "        # Column Selection button\n",
    "        self._bcs = widgets.Button(layout=widgets.Layout(width='500px',height='50px'))\n",
    "        self._bcs.description = \"Choose Columns\"\n",
    "        self._bcs.on_click(self.choose_columns)\n",
    "        self._bcs.disabled = True\n",
    "        \n",
    "        # define layout\n",
    "        box_layout = widgets.Layout(display='center',\n",
    "                            flex_flow='row',\n",
    "                            align_items='center',\n",
    "                            border='None',\n",
    "                            justify_content='space-between',\n",
    "                            width='900px')\n",
    "        \n",
    "        self._box = widgets.Box(children=[self._bsf, self._dds, self._blj,\n",
    "                                          self._csm, self._bcs], layout=box_layout, main_size = 10)\n",
    "        display(self._box)\n",
    "        \n",
    "    def select_excel(self,b): \n",
    "        # Create Tk root\n",
    "        root = Tk()\n",
    "        # Hide the main window\n",
    "        root.withdraw()\n",
    "        # Raise the root to the top of all windows.\n",
    "        root.call('wm', 'attributes', '.', '-topmost', True)\n",
    "        \n",
    "        new_file = filedialog.askopenfilename(multiple=True,filetypes = [(\"XLS\",'*.xls')])\n",
    "        if new_file != \"\":\n",
    "            self._bsf.files = new_file\n",
    "            self._bsf.description = \"File Selected\"\n",
    "            self._bsf.icon = \"check-square-o\"\n",
    "            self._bsf.button_style = \"success\"\n",
    "            self._blj.disabled = False\n",
    "            print('Selected file: {}'.format(self._bsf.files))\n",
    "            file_path = self._bsf.files\n",
    "            self._excel = pd.ExcelFile(file_path[0], on_demand = True)\n",
    "            self._sheets = self._excel.sheet_names\n",
    "            self._dds.options = self._sheets\n",
    "            self._dds.disabled = False\n",
    "\n",
    "    def load_excel(self, b):\n",
    "        self._dataframe = pd.read_excel(self._bsf.files[0], sheet_name = self._dds.value) \n",
    "        self._blj.button_style = \"success\"\n",
    "        display(self._dataframe.head())\n",
    "        self._csm.disabled = False\n",
    "        self._csm.options = list(self._dataframe.columns.values)\n",
    "        self._csm.disabled = False\n",
    "        self._bcs.disabled = False\n",
    "    \n",
    "    def choose_columns(self, b):\n",
    "        self._documents = list()\n",
    "        for element in self._csm.value:\n",
    "            self._dataframe[element].fillna(\"\", inplace=True)\n",
    "            self._documents.append( self._dataframe[element])\n",
    "        self._documents = join_series(self._documents)\n",
    "        print(self._documents)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3853072c1f74170a6cfcffef6ef9216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Button(description='Select File', icon='square-o', layout=Layout(height='50px', width='500px'), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = Display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = CountVectorizer(tokenizer=tokenization_process)\n",
    "count_matrix = counter.fit_transform(d._documents)\n",
    "terms = np.array(counter.get_feature_names())\n",
    "index = np.array(range(len(terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionario = pd.DataFrame.from_csv('Dicionario.csv')\n",
    "dicionario.reset_index(inplace = True)\n",
    "vec_dicionario = dicionario['Words'].values\n",
    "for index, element in enumerate(vec_dicionario):\n",
    "    vec_dicionario[index] = unidecode.unidecode(element.lower())\n",
    "vec_dicionario = np.unique(vec_dicionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_checker(count_matrix, terms, repeats=100):\n",
    "    #count_matrix em sparse\n",
    "    #terms em np.array\n",
    "    if len(terms) != count_matrix.shape[1]:\n",
    "        raise ValueError(\"Terms and Count Matrix are not a match.\")\n",
    "    else:\n",
    "        index_terms = np.array(range(len(terms)))\n",
    "        token_frequency = count_matrix.sum(0)\n",
    "        existent_words = terms[(token_frequency>repeats).A1]\n",
    "        existent_words_index = index_terms[(token_frequency>repeats).A1]\n",
    "        dubious_words = terms[(token_frequency<=repeats).A1]\n",
    "        dubious_words_index = index_terms[(token_frequency<=repeats).A1]\n",
    "        inexistent_words = np.array([])\n",
    "        inexistent_words_index = np.array([])\n",
    "        for number, term in enumerate(dubious_words):\n",
    "            if term in vec_dicionario:\n",
    "                existent_words = np.append(existent_words, term)\n",
    "                existent_words_index = np.append(existent_words_index, dubious_words_index[number])\n",
    "            else:\n",
    "                inexistent_words = np.append(inexistent_words, term)\n",
    "                inexistent_words_index = np.append(inexistent_words_index, dubious_words_index[number])\n",
    "        return existent_words, inexistent_words, existent_words_index, inexistent_words_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "existent_words, inexistent_words, existent_words_index, inexistent_words_index =  word_checker(count_matrix, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1510.5069901943207\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "begin = time.time()\n",
    "edit_distance_matrix = [[edit_distance_nltk.edit_distance(i, j) for i in existent_words] for j in inexistent_words]\n",
    "print(time.time()-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_distance_matrix = np.array(edit_distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_switcheroo = list()\n",
    "for index_old_word, inexistent_word_line in enumerate(edit_distance_matrix):\n",
    "    if np.min(inexistent_word_line) < 3:\n",
    "        \n",
    "        if len(inexistent_word_line[inexistent_word_line == np.min(inexistent_word_line)]) > 1:\n",
    "            max_freq_check = existent_words_index[inexistent_word_line == np.min(inexistent_word_line)]\n",
    "            count_matrix_sums = [count_matrix[:,element_freq_check].sum() for element_freq_check in max_freq_check]\n",
    "            index_new_word = max_freq_check[np.argmax(count_matrix_sums)]\n",
    "            word_switcheroo.append([inexistent_words[index_old_word], terms[index_new_word]])\n",
    "        else:\n",
    "            word_switcheroo.append([inexistent_words[index_old_word], terms[existent_words_index[inexistent_word_line == np.min(inexistent_word_line)]][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_words = list()\n",
    "right_words = list()\n",
    "for mini_list in word_switcheroo:\n",
    "    wrong_words.append(mini_list[0])\n",
    "    right_words.append(mini_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_process_removing_errors(string):\n",
    "    \"\"\"tokenizes one string, removing not used punctuation in portuguese language in the process\"\"\"\n",
    "    aux = tokenizer(string)\n",
    "    output = list()\n",
    "    for index, word in enumerate(aux):\n",
    "        if re.search('([A-Za-zÀ-ÿ]+(-|)[A-Za-zÀ-ÿ]+|[A-Za-zÀ-ÿ]+)', str(word)):\n",
    "            if unidecode.unidecode(str(re.search('([A-Za-zÀ-ÿ]+(-|)[A-Za-zÀ-ÿ]+|[A-Za-zÀ-ÿ]+)', str(word)).group(0)).lower()) in wrong_words:\n",
    "                output.append(right_words[wrong_words ==unidecode.unidecode(str(re.search('([A-Za-zÀ-ÿ]+(-|)[A-Za-zÀ-ÿ]+|[A-Za-zÀ-ÿ]+)', str(word)).group(0)).lower())])\n",
    "            else:\n",
    "                output.append(unidecode.unidecode(str(re.search('([A-Za-zÀ-ÿ]+(-|)[A-Za-zÀ-ÿ]+|[A-Za-zÀ-ÿ]+)', str(word)).group(0)).lower()))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8037, 10175)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5009, 5166)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #dataframes e series\n",
    "import matplotlib.pyplot as plt #graphics\n",
    "import numpy as np #matrixes\n",
    "import nltk #Tokenize\n",
    "import sklearn as skl\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer #tf-idf; tf\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT\n",
    "TFIDF = TfidfVectorizer(tokenizer = tokenization_process_removing_errors)\n",
    "input_tfidf = TFIDF.fit_transform(d._documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#OUTPUT\n",
    "output_wo_type = d._dataframe['Tipo Trabalho']\n",
    "output_wo_values = output_wo_type.values\n",
    "output_wo_values[output_wo_values =='GAR'] = 'MC'\n",
    "output_wo_values[output_wo_values =='AN'] = 'MPR'\n",
    "output_wo_values[output_wo_values =='ML'] = 'MPR'\n",
    "output_wo_values[output_wo_values =='RP'] = 'MC'\n",
    "output_wo_values[output_wo_values =='GPA'] = 'MPR'\n",
    "output_wo_values[output_wo_values =='MPR'] = 1\n",
    "output_wo_values[output_wo_values =='MC'] = 0\n",
    "labels = pd.Series(data = output_wo_values, index = range(len(output_wo_values)),dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfolder = skl.model_selection.KFold(n_splits=10,shuffle = True, random_state= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureselect(Model, X, Y):\n",
    "    Model.fit(X, Y)\n",
    "    print('Model Fited')\n",
    "    featureselectmodel = SelectFromModel(Model, prefit = True)\n",
    "    NewX = featureselectmodel.transform(X)\n",
    "    return NewX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_score(Model,X,Y, cores = -1):\n",
    "    return skl.model_selection.cross_val_score(Model, X, Y, cv = Kfolder, n_jobs = cores, scoring= 'f1').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(Model, X, Y):\n",
    "    Score = list()\n",
    "    for ind, column in enumerate(col_labels):\n",
    "        Score.append(cross_validation_score(Model, X, Y[ind]))\n",
    "        print('Score Created')\n",
    "    print(sum(Score)/len(Score))\n",
    "    return Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBSVM Input\n",
    "def pr(y_i, y, x):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "def get_input_nbsvm(y, x):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y,x) / pr(0,y,x))\n",
    "    x_nb = x.multiply(r)\n",
    "    return x_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_data = [input_tfidf,count_matrix,cbtw_matrix]\n",
    "input_tfidf_scores = list()\n",
    "count_matrix_scores = list()\n",
    "cbtw_matrix_scores = list()\n",
    "for iteration in range(0,11):\n",
    "    for index, data in enumerate(pre_processed_data):\n",
    "        intermediate_data = data\n",
    "        Kfolder = skl.model_selection.KFold(n_splits=10,shuffle = True, random_state= iteration)\n",
    "        model_input = get_input_nbsvm(labels, intermediate_data)\n",
    "        score = cross_validation_score(SVM, model_input, labels)\n",
    "        if index % 3 == 0:\n",
    "            input_tfidf_scores.append(score)\n",
    "        elif index % 3 == 1:\n",
    "            count_matrix_scores.append(score)\n",
    "        elif index % 3 == 2:\n",
    "            cbtw_matrix_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_treated = TfidfVectorizer(tokenizer = tokenization_process_removing_errors)\n",
    "TF_IDF_untreated = TfidfVectorizer(tokenizer = tokenization_process)\n",
    "input_tfidf_treated = TF_IDF_treated.fit_transform(d._documents)\n",
    "input_tfidf_untreated = TF_IDF_untreated.fit_transform(d._documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8807025452397087\n",
      "0.8923201764272857\n"
     ]
    }
   ],
   "source": [
    "model_input_treated = get_input_nbsvm(labels, input_tfidf_treated)\n",
    "model_input_untreated = get_input_nbsvm(labels, input_tfidf_untreated)\n",
    "score_list_treated = list()\n",
    "score_list_untreated = list()\n",
    "for iteration in range(0,11):\n",
    "    Kfolder = skl.model_selection.KFold(n_splits=10,shuffle = True, random_state= iteration)\n",
    "    score_list_treated.append(cross_validation_score(SVM, model_input_treated, labels))\n",
    "    score_list_untreated.append(cross_validation_score(SVM, model_input_untreated, labels))\n",
    "print(np.mean(score_list_treated))\n",
    "print(np.mean(score_list_untreated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbsvm_input = get_input_nbsvm(labels, input_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8808536309617079"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_score(SVM, nbsvm_input, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(input_tfidf_scores))\n",
    "print(np.mean(count_matrix_scores))\n",
    "print(np.mean(cbtw_matrix_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbsvm_input = get_mdl(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_score(SVM, nbsvm_input, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "TREE = DecisionTreeClassifier()\n",
    "LOG = LogisticRegression()\n",
    "SVM = skl.svm.LinearSVC()\n",
    "NB = MultinomialNB()\n",
    "MLP = MLPClassifier(hidden_layer_sizes=(5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [TREE, LOG, SVM, NB, MLP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9241449178233834"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_score(TREE, cbtw_matrix, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,11):\n",
    "    print(cross_validation_score(MLPClassifier(hidden_layer_sizes=(i,),random_state=100), input_tfidf, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Class' from 'C:\\\\Users\\\\Sérgio Jesus\\\\Documents\\\\Sergio2018\\\\Class.py'>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Class\n",
    "import importlib\n",
    "importlib.reload(Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Document_Dataframe = Class.DocumentDataFrame(d._documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "cbtw_matrix = Document_Dataframe.cbtw_matrix(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl.svm.libsvm.cross_validation(input_tfidf, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_list = np.array(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('comparison_list.npy', new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_match = np.argmin(new_list, axis = 0)\n",
    "for inexistent, existent in enumerate(best_match):\n",
    "    Correction_id = np.array()\n",
    "    print(inexistent)\n",
    "    print(inexistent_words[inexistent])\n",
    "    print(existent_words[existent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_switch = np.array([[inexistent_words_index[inexistent].astype(int),existent_words_index[existent].astype(int)]\n",
    "                       for inexistent, existent in enumerate(best_match)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "id_switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_error_remover(documents):\n",
    "    timevariable = time.time()\n",
    "    counter = CountVectorizer(tokenizer=tokenization_process)\n",
    "    print(time.time()-timevariable)\n",
    "    timevariable = time.time()\n",
    "    count_matrix = counter.fit_transform(documents)\n",
    "    print(time.time()-timevariable)\n",
    "    timevariable = time.time()\n",
    "    terms = np.array(counter.get_feature_names())\n",
    "    print(time.time()-timevariable)\n",
    "    timevariable = time.time()\n",
    "    vec_dictionary = create_dictionary_nparray('Dicionario.csv')\n",
    "    print(time.time()-timevariable)\n",
    "    timevariable = time.time()\n",
    "    existent_words, inexistent_words, existent_words_index, inexistent_words_index =  word_checker(count_matrix, terms)\n",
    "    print(time.time()-timevariable)\n",
    "    timevariable = time.time()\n",
    "    distance_matrix = edit_distance_matrix(inexistent_words, existent_words) #bottleneck\n",
    "    print(time.time()-timevariable)\n",
    "    timevariable = time.time()\n",
    "    id_switch = id_switcher_matrix(distance_matrix, existent_words_index, inexistent_words_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance_matrix(first_term_vector, second_term_vector): #bottleneck\n",
    "    return np.array([[edit_distance_nltk.edit_distance(i, j) for i in first_term_vector] for j in second_term_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_switcher_matrix(distance_matrix, inexistent_words_index, existent_words_index):\n",
    "    best_match = np.argmin(distance_matrix, axis = 0)\n",
    "    id_switch = np.array([[inexistent_words_index[inexistent].astype(int),existent_words_index[existent].astype(int)]\n",
    "                       for inexistent, existent in enumerate(best_match)])\n",
    "    return id_switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_nparray(path):\n",
    "    dictionary = pd.DataFrame.from_csv(path)\n",
    "    dictionary.reset_index(inplace = True)\n",
    "    vec_dictionary = dicionario['Words'].values\n",
    "    for index, element in enumerate(vec_dictionary):\n",
    "        vec_dictionary[index] = unidecode.unidecode(element.lower())\n",
    "    return vec_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['aaeronave', 'aeronave'],\n",
       " ['aaf', 'aa'],\n",
       " ['aalrme', 'alarme'],\n",
       " ['aarme', 'alarme'],\n",
       " ['aauto-level', 'auto-level'],\n",
       " ['abanado', 'abana'],\n",
       " ['abandonamos', 'abandonado'],\n",
       " ['abandonei', 'abandonar'],\n",
       " ['abanou', 'acabou'],\n",
       " ['abatimentos', 'abatimento'],\n",
       " ['abolsa', 'bolsa'],\n",
       " ['about', 'aut'],\n",
       " ['above', 'abre'],\n",
       " ['abracadeiras', 'bracadeira'],\n",
       " ['abrandou', 'arrancou'],\n",
       " ['abri-la', 'abril'],\n",
       " ['abria', 'abrir'],\n",
       " ['abriam', 'abrir'],\n",
       " ['acabando', 'acabado'],\n",
       " ['acabarem', 'acabar'],\n",
       " ['acabava', 'acabado'],\n",
       " ['acabe', 'acaba'],\n",
       " ['acaitacao', 'aceitacao'],\n",
       " ['acamponhada', 'acompanhada'],\n",
       " ['accedez', 'aceder'],\n",
       " ['access', 'acesso'],\n",
       " ['accionando', 'accionado'],\n",
       " ['acederam', 'aceder'],\n",
       " ['acedi', 'acende'],\n",
       " ['acedido', 'cedido'],\n",
       " ['aceesso', 'acesso'],\n",
       " ['aceiracao', 'aceitacao'],\n",
       " ['aceit', 'aceite'],\n",
       " ['aceitaao', 'aceitacao'],\n",
       " ['aceitacaode', 'aceitacao'],\n",
       " ['aceitacap', 'aceitacao'],\n",
       " ['aceitaco', 'aceitacao'],\n",
       " ['aceitacoes', 'aceitamos'],\n",
       " ['aceitada', 'aceitado'],\n",
       " ['aceitao', 'aceitar'],\n",
       " ['aceitaram', 'aceitacao'],\n",
       " ['aceitarem', 'aceitar'],\n",
       " ['aceitaro', 'aceitar'],\n",
       " ['aceitava', 'aceitacao'],\n",
       " ['aceitei', 'aceite'],\n",
       " ['aceitsacao', 'aceitacao'],\n",
       " ['aceitye', 'aceite'],\n",
       " ['acendem', 'acende'],\n",
       " ['acentuados', 'acentuado'],\n",
       " ['acesas', 'apesar'],\n",
       " ['acessa', 'acesso'],\n",
       " ['acetacao', 'aceitacao'],\n",
       " ['achando', 'armando'],\n",
       " ['acharem', 'achar'],\n",
       " ['achegada', 'chegada'],\n",
       " ['acionou', 'adicionou'],\n",
       " ['acitacao', 'aceitacao'],\n",
       " ['acitar', 'aceitar'],\n",
       " ['acom', 'com'],\n",
       " ['acomanhamento', 'acompanhamento'],\n",
       " ['acomapanhamento', 'acompanhamento'],\n",
       " ['acompahamento', 'acompanhamento'],\n",
       " ['acompanahr', 'acompanhar'],\n",
       " ['acompanhamos', 'acompanhados'],\n",
       " ['acompanharam', 'acompanhar'],\n",
       " ['acompanharmos', 'acompanhados'],\n",
       " ['acompanhei', 'acompanhar'],\n",
       " ['acompanhou', 'acompanhar'],\n",
       " ['acomulacao', 'acumulacao'],\n",
       " ['acondicionado', 'condicionado'],\n",
       " ['aconselho', 'conselho'],\n",
       " ['acopanhamento', 'acompanhamento'],\n",
       " ['acopolada', 'acoplada'],\n",
       " ['acopologem', 'acoplagem'],\n",
       " ['acopstagem', 'acostagem'],\n",
       " ['acosta', 'acostar'],\n",
       " ['acostadas', 'acostada'],\n",
       " ['acostado', 'acostada'],\n",
       " ['acostag', 'acostar'],\n",
       " ['acostage', 'acostagem'],\n",
       " ['acostagens', 'acostagem'],\n",
       " ['acostamentos', 'acostamento'],\n",
       " ['acostei', 'acostar'],\n",
       " ['acostou', 'agosto'],\n",
       " ['acostou-se', 'encostou-se'],\n",
       " ['acrilica', 'acrilico'],\n",
       " ['acrilicos', 'acrilico'],\n",
       " ['acs', 'as'],\n",
       " ['actaudo', 'actuado'],\n",
       " ['actctuado', 'actuado'],\n",
       " ['actiivo', 'activo'],\n",
       " ['action', 'activo'],\n",
       " ['activacao', 'aceitacao'],\n",
       " ['activada', 'actuada'],\n",
       " ['activado', 'actuado'],\n",
       " ['activando', 'actuando'],\n",
       " ['activas', 'activos'],\n",
       " ['activava', 'activa'],\n",
       " ['active', 'activo'],\n",
       " ['activi', 'activo'],\n",
       " ['activou', 'activo'],\n",
       " ['actua-se', 'actuais'],\n",
       " ['actuacaop', 'actuacao'],\n",
       " ['actuacoes', 'situacoes'],\n",
       " ['actuadas', 'actuada'],\n",
       " ['actuadops', 'actuado'],\n",
       " ['actuador', 'actuado'],\n",
       " ['actuados', 'actuado'],\n",
       " ['actualizacao', 'actualizada'],\n",
       " ['actuamos', 'actuado'],\n",
       " ['actuasse', 'acabasse'],\n",
       " ['actuava', 'actuada'],\n",
       " ['actuda', 'actuada'],\n",
       " ['actue', 'actua'],\n",
       " ['actuei', 'actuar'],\n",
       " ['actuou', 'actuar'],\n",
       " ['actuvado', 'actuado'],\n",
       " ['actvo', 'activo'],\n",
       " ['acuado', 'actuado'],\n",
       " ['acusava', 'acusa'],\n",
       " ['acustagem', 'acostagem'],\n",
       " ['acutacao', 'aceitacao'],\n",
       " ['acutado', 'actuado'],\n",
       " ['acveitacao', 'aceitacao'],\n",
       " ['adctivo', 'activo'],\n",
       " ['additional', 'adicional'],\n",
       " ['ade', 'de'],\n",
       " ['ademar', 'apesar'],\n",
       " ['adenda', 'ainda'],\n",
       " ['adge', 'edge'],\n",
       " ['adicionado', 'accionado'],\n",
       " ['adjudicado', 'adjudicacao'],\n",
       " ['adm', 'ada'],\n",
       " ['adviser', 'avisar'],\n",
       " ['aea', 'area'],\n",
       " ['aeitacao', 'aceitacao'],\n",
       " ['aereas', 'areas'],\n",
       " ['aernave', 'aeronave'],\n",
       " ['aeroanve', 'aeronave'],\n",
       " ['aerodrome', 'aerodromo'],\n",
       " ['aerogare', 'aeronave'],\n",
       " ['aeronane', 'aeronave'],\n",
       " ['aeronav', 'aeronave'],\n",
       " ['aeronaver', 'aeronave'],\n",
       " ['aeronavwe', 'aeronave'],\n",
       " ['aerone', 'aeronave'],\n",
       " ['aeroneve', 'aeronave'],\n",
       " ['aeronve', 'aeronave'],\n",
       " ['aeropuerto', 'aeroporto'],\n",
       " ['aertonave', 'aeronave'],\n",
       " ['aesta', 'esta'],\n",
       " ['af', 'a'],\n",
       " ['afastamendo', 'afastamento'],\n",
       " ['afectou', 'afectos'],\n",
       " ['afeira', 'feira'],\n",
       " ['afetada', 'afectada'],\n",
       " ['afetadas', 'afectada'],\n",
       " ['afetados', 'afectados'],\n",
       " ['afetar', 'afectar'],\n",
       " ['afeto', 'afecto'],\n",
       " ['afetou', 'aceitou'],\n",
       " ['afinada', 'afinado'],\n",
       " ['afinados', 'afinado'],\n",
       " ['afr', 'ar'],\n",
       " ['afrente', 'frente'],\n",
       " ['after', 'ter'],\n",
       " ['again', 'agir'],\n",
       " ['agendada', 'agendadas'],\n",
       " ['agendado', 'agendadas'],\n",
       " ['agendados', 'agendadas'],\n",
       " ['agent', 'agente'],\n",
       " ['agido', 'sido'],\n",
       " ['agot', 'ao'],\n",
       " ['agradeciamos', 'agradecemos'],\n",
       " ['agradeco', 'agrado'],\n",
       " ['agravada', 'gravada'],\n",
       " ['aguarda', 'aguardar'],\n",
       " ['aguardada', 'aguardar'],\n",
       " ['aguardado', 'guardado'],\n",
       " ['aguardaram', 'aguardar'],\n",
       " ['aguardarei', 'aguardar'],\n",
       " ['aguardava', 'aguardar'],\n",
       " ['aguardei', 'aguardar'],\n",
       " ['aguardo', 'aquando'],\n",
       " ['aguardou', 'guardou'],\n",
       " ['aida', 'saida'],\n",
       " ['airbus', 'virus'],\n",
       " ['ajc', 'ac'],\n",
       " ['ajcavaco', 'cavaco'],\n",
       " ['ajpinho', 'pinho'],\n",
       " ['ajudei', 'ajuda'],\n",
       " ['ajustadas', 'ajustada'],\n",
       " ['ajustados', 'ajustado'],\n",
       " ['aklarme', 'alarme'],\n",
       " ['alaeme', 'alarme'],\n",
       " ['alaimentacao', 'alimentacao'],\n",
       " ['alame', 'alarme'],\n",
       " ['alames', 'alarmes'],\n",
       " ['alamre', 'alarme'],\n",
       " ['alamres', 'alarmes'],\n",
       " ['alare', 'alarme'],\n",
       " ['alarem', 'alarme'],\n",
       " ['alareme', 'alarme'],\n",
       " ['alarems', 'alarme'],\n",
       " ['alarm', 'alarme'],\n",
       " ['alarmea', 'alarme'],\n",
       " ['alarmme', 'alarme'],\n",
       " ['alarmne', 'alarme'],\n",
       " ['alarne', 'alarme'],\n",
       " ['alarrme', 'alarme'],\n",
       " ['alarrmes', 'alarmes'],\n",
       " ['alatme', 'alarme'],\n",
       " ['albano', 'plano'],\n",
       " ['alberto', 'aberto'],\n",
       " ['alerme', 'alarme'],\n",
       " ['alertadas', 'apertadas'],\n",
       " ['alertado', 'apertado'],\n",
       " ['alertados', 'apertado'],\n",
       " ['alertamos', 'aceitamos'],\n",
       " ['alertando', 'apertado'],\n",
       " ['alertei', 'alerta'],\n",
       " ['alerto', 'aberto'],\n",
       " ['alfaia', 'alfa'],\n",
       " ['algusn', 'algum'],\n",
       " ['alimentadas', 'alimentados'],\n",
       " ['alimentadores', 'alimentados'],\n",
       " ['alimentara', 'alimentar'],\n",
       " ['alimentcao', 'alimentacao'],\n",
       " ['alimetacao', 'alimentacao'],\n",
       " ['alinhadas', 'alinhada'],\n",
       " ['alonso', 'longo'],\n",
       " ['alrames', 'alarmes'],\n",
       " ['alrarme', 'alarme'],\n",
       " ['alrme', 'alarme'],\n",
       " ['alrmes', 'alarmes'],\n",
       " ['alsaer', 'alar'],\n",
       " ['alsman', 'alma'],\n",
       " ['also', 'als'],\n",
       " ['alspsp', 'alssup'],\n",
       " ['alsrme', 'alarme'],\n",
       " ['alssoa', 'alssup'],\n",
       " ['alssoc', 'alssup'],\n",
       " ['alssupma', 'alssupman'],\n",
       " ['alssusoa', 'alssupsoa'],\n",
       " ['alster', 'master'],\n",
       " ['alsupsoa', 'alssupsoa'],\n",
       " ['alteral', 'alterar'],\n",
       " ['altered', 'alterado'],\n",
       " ['alterei', 'alterar'],\n",
       " ['alternando', 'alterado'],\n",
       " ['alves', 'als'],\n",
       " ['am', 'a'],\n",
       " ['amais', 'mais'],\n",
       " ['amanga', 'manga'],\n",
       " ['amaral', 'camara'],\n",
       " ['amaro', 'mario'],\n",
       " ['americo', 'afericao'],\n",
       " ['amesma', 'mesma'],\n",
       " ['amnaga', 'apaga'],\n",
       " ['amnga', 'manga'],\n",
       " ['amp', 'a'],\n",
       " ['amribeiro', 'ribeiro'],\n",
       " ['amt', 'cmt'],\n",
       " ['analisada', 'analisado'],\n",
       " ['analisei', 'analise'],\n",
       " ['anamolia', 'anomalia'],\n",
       " ['anaso', 'acaso'],\n",
       " ['ancostada', 'encostada'],\n",
       " ['andrade', 'andre'],\n",
       " ['andreia', 'andre'],\n",
       " ['anexam', 'anexa'],\n",
       " ['anexas', 'anexos'],\n",
       " ['angel', 'anel'],\n",
       " ['angle', 'angulo'],\n",
       " ['animalia', 'anomalia'],\n",
       " ['anobra', 'manobra'],\n",
       " ['anolmalia', 'anomalia'],\n",
       " ['anom', 'ano'],\n",
       " ['anomalais', 'anomalia'],\n",
       " ['anomlaia', 'anomalia'],\n",
       " ['anomlia', 'anomalia'],\n",
       " ['anomnalia', 'anomalia'],\n",
       " ['anomola', 'anomala'],\n",
       " ['anteruiormente', 'anteriormente'],\n",
       " ['anti-panico', 'antipanico'],\n",
       " ['antolin', 'antonio'],\n",
       " ['antunes', 'antes'],\n",
       " ['anulada', 'aguada'],\n",
       " ['anutencao', 'manutencao'],\n",
       " ['any', 'and'],\n",
       " ['aoa', 'ao'],\n",
       " ['aofazerem', 'fazerem'],\n",
       " ['aomalia', 'anomalia'],\n",
       " ['aooperador', 'operador'],\n",
       " ['aoos', 'apos'],\n",
       " ['aoparque', 'parque'],\n",
       " ['aoperacao', 'operacao'],\n",
       " ['aoto-level', 'auto-level'],\n",
       " ['ap', 'a'],\n",
       " ['apagaram', 'apagadas'],\n",
       " ['aparafusadas', 'aparafusado'],\n",
       " ['apareceram', 'aparecer'],\n",
       " ['aparecia', 'apareca'],\n",
       " ['apertados', 'apertado'],\n",
       " ['api', 'apis'],\n",
       " ['aplaca', 'placa'],\n",
       " ['apo', 'ao'],\n",
       " ['aponte', 'ponte'],\n",
       " ['aporta', 'porta'],\n",
       " ['apra', 'pra'],\n",
       " ['apresentamos', 'apresentados'],\n",
       " ['apresentava', 'apresentada'],\n",
       " ['apresentavam', 'apresentaram'],\n",
       " ['apresento', 'apresenta'],\n",
       " ['apron', 'apos'],\n",
       " ['aprovada', 'aprovacao'],\n",
       " ['aprox', 'apos'],\n",
       " ['aproximava', 'aproximacao'],\n",
       " ['apu', 'au'],\n",
       " ['apur', 'por'],\n",
       " ['aq', 'a'],\n",
       " ['aquel', 'aquele'],\n",
       " ['araujo', 'arranjo'],\n",
       " ['arcondicionado', 'condicionado'],\n",
       " ['areeiro', 'carneiro'],\n",
       " ['areira', 'area'],\n",
       " ['arise', 'apis'],\n",
       " ['arme', 'are'],\n",
       " ['armou', 'parou'],\n",
       " ['arnadura', 'armadura'],\n",
       " ['arnaldo', 'armando'],\n",
       " ['arramque', 'arranque'],\n",
       " ['arrancadas', 'arrancada'],\n",
       " ['arrancador', 'arrancado'],\n",
       " ['arrancados', 'arrancado'],\n",
       " ['arrancavam', 'arrancaram'],\n",
       " ['arrival', 'arraial'],\n",
       " ['arrive', 'drive'],\n",
       " ['artur', 'altura'],\n",
       " ['arturc', 'altura'],\n",
       " ['asap', 'asa'],\n",
       " ['ask', 'as'],\n",
       " ['aslarme', 'alarme'],\n",
       " ['aslsupman', 'alssupman'],\n",
       " ['assegurado', 'assegurar'],\n",
       " ['assegurarao', 'assegurar'],\n",
       " ['asseitacao', 'aceitacao'],\n",
       " ['assentiu', 'assentou'],\n",
       " ['assinalados', 'assinalado'],\n",
       " ['assistant', 'assistente'],\n",
       " ['assistencias', 'assistencia'],\n",
       " ['assisti', 'assistir'],\n",
       " ['assistida', 'assistido'],\n",
       " ['assumi', 'assume'],\n",
       " ['assumia', 'assumiu'],\n",
       " ['atarso', 'atraso'],\n",
       " ['atendeu', 'acende'],\n",
       " ['atendia', 'atendido'],\n",
       " ['atendiam', 'atendido'],\n",
       " ['atendida', 'atendido'],\n",
       " ['atengiu', 'atingiu'],\n",
       " ['atentos', 'tentou'],\n",
       " ['ates', 'ate'],\n",
       " ['atingia', 'atingir'],\n",
       " ['atingidos', 'atingido'],\n",
       " ['ativa', 'activa'],\n",
       " ['ativacao', 'afinacao'],\n",
       " ['ativada', 'tirada'],\n",
       " ['ativado', 'afinado'],\n",
       " ['atividades', 'actividades'],\n",
       " ['ativo', 'activo'],\n",
       " ['ativos', 'activos'],\n",
       " ['ativou', 'activo'],\n",
       " ['atmf', 'tme'],\n",
       " ['atp', 'tp'],\n",
       " ['atracava', 'tratava'],\n",
       " ['atrasaram', 'atrasar'],\n",
       " ['atravez', 'atraves'],\n",
       " ['atraz', 'atras'],\n",
       " ['atrazo', 'atraso'],\n",
       " ['atrazou', 'atrasou'],\n",
       " ['atribua', 'atribuir'],\n",
       " ['atuacao', 'actuacao'],\n",
       " ['atuada', 'actuada'],\n",
       " ['atuado', 'actuado'],\n",
       " ['atuados', 'actuado'],\n",
       " ['atualmente', 'actualmente'],\n",
       " ['atuar', 'actuar'],\n",
       " ['atuou', 'andou'],\n",
       " ['aui', 'aqui'],\n",
       " ['auito', 'auto'],\n",
       " ['aujto', 'auto'],\n",
       " ['aulevel', 'level'],\n",
       " ['aunivelamento', 'autonivelamento'],\n",
       " ['auo-leve', 'auto-level'],\n",
       " ['auotlevel', 'auto-level'],\n",
       " ['aurtolevel', 'autolevel'],\n",
       " ['ausentar', 'ausente'],\n",
       " ['author', 'autor'],\n",
       " ['auticlismo', 'autoclismo'],\n",
       " ['autm', 'auto'],\n",
       " ['auto-leve', 'auto-level'],\n",
       " ['auto-levell', 'auto-level'],\n",
       " ['auto-levels', 'auto-level'],\n",
       " ['auto-nevelamento', 'autonivelamento'],\n",
       " ['auto-nivel', 'auto-level'],\n",
       " ['auto-nivelamento', 'autonivelamento'],\n",
       " ['autocolantes', 'autocolante'],\n",
       " ['autocriacao', 'autorizacao'],\n",
       " ['autoi', 'auto'],\n",
       " ['autolel', 'autolevel'],\n",
       " ['autolivelamento', 'autonivelamento'],\n",
       " ['automatic', 'automatico'],\n",
       " ['automaticamento', 'automaticamente'],\n",
       " ['automatismo', 'automatico'],\n",
       " ['autonivel', 'autolevel'],\n",
       " ['autonivelamenrto', 'autonivelamento'],\n",
       " ['autonivelamneto', 'autonivelamento'],\n",
       " ['autonivelemanto', 'autonivelamento'],\n",
       " ['autonivelemento', 'autonivelamento'],\n",
       " ['autonuivelamento', 'autonivelamento'],\n",
       " ['autonuvelamento', 'autonivelamento'],\n",
       " ['autop-level', 'auto-level'],\n",
       " ['autorizacoes', 'autorizados'],\n",
       " ['autorizadas', 'autorizados'],\n",
       " ['autorizou', 'autorizar'],\n",
       " ['aux', 'aut'],\n",
       " ['auxiliada', 'auxiliar'],\n",
       " ['auxiliado', 'auxilio'],\n",
       " ['av', 'a'],\n",
       " ['avac', 'cavaco'],\n",
       " ['avaliados', 'avariado'],\n",
       " ['avaliou', 'avancou'],\n",
       " ['avancamos', 'avancado'],\n",
       " ['avancaram', 'avancar'],\n",
       " ['avancavam', 'avancava'],\n",
       " ['avancei', 'avanco'],\n",
       " ['avant', 'avanco'],\n",
       " ['avariada', 'avariado'],\n",
       " ['avariadas', 'avariado'],\n",
       " ['avariados', 'avariado'],\n",
       " ['avarias', 'avaria'],\n",
       " ['avaruia', 'avaria'],\n",
       " ['avaruiia', 'avaria'],\n",
       " ['averigue', 'averiguar'],\n",
       " ['avisada', 'avisado'],\n",
       " ['avisadas', 'avisado'],\n",
       " ['avisem', 'aviso'],\n",
       " ['avisou', 'aviso'],\n",
       " ['aviz', 'avis'],\n",
       " ['avria', 'avaria'],\n",
       " ['aware', 'alarme'],\n",
       " ['awe', 'ate'],\n",
       " ['awg', 'ag'],\n",
       " ['backup', 'back'],\n",
       " ['bagaem', 'bagagem'],\n",
       " ['bai', 'sai'],\n",
       " ['baiada', 'saida'],\n",
       " ['baias', 'baia'],\n",
       " ['baixa-la', 'baixava'],\n",
       " ['baixa-se', 'baixas'],\n",
       " ['baixada', 'baixava'],\n",
       " ['baixamos', 'baixado'],\n",
       " ['baixou-se', 'deixou-se'],\n",
       " ['balancas', 'balanca'],\n",
       " ['balastros', 'balastro'],\n",
       " ['baldes', 'balde'],\n",
       " ['balizada', 'realizada'],\n",
       " ['balizado', 'realizado'],\n",
       " ['balnarios', 'balnearios'],\n",
       " ['bamper', 'bumper'],\n",
       " ['banner', 'manter'],\n",
       " ['barajas', 'barata'],\n",
       " ['barramento', 'arruamento'],\n",
       " ['barramentos', 'arruamentos'],\n",
       " ['barros', 'barras'],\n",
       " ['barrulhos', 'barulho'],\n",
       " ['bastava', 'estava'],\n",
       " ['bastou', 'passou'],\n",
       " ['batista', 'baptista'],\n",
       " ['batteries', 'baterias'],\n",
       " ['baumper', 'bumper'],\n",
       " ['bay-pass', 'bypass'],\n",
       " ['baypass', 'bypass'],\n",
       " ['bch', 'h'],\n",
       " ['bci', 'ci'],\n",
       " ['be', 'de'],\n",
       " ['been', 'bem'],\n",
       " ['before', 'refere'],\n",
       " ['belden', 'balde'],\n",
       " ['below', 'pelo'],\n",
       " ['belt', 'pelo'],\n",
       " ['bemper', 'bumper'],\n",
       " ['beneficiadas', 'beneficiacao'],\n",
       " ['besam', 'bem'],\n",
       " ['best', 'rest'],\n",
       " ['betoneira', 'botoneira'],\n",
       " ['beumer', 'bumper'],\n",
       " ['bff', 'boa'],\n",
       " ['bg', 'b'],\n",
       " ['bgt', 'bit'],\n",
       " ['big', 'bag'],\n",
       " ['biletes', 'bilhetes'],\n",
       " ['billetes', 'bilhetes'],\n",
       " ['bipolar', 'isolar'],\n",
       " ['biumper', 'bumper'],\n",
       " ['bl-t', 'lt'],\n",
       " ['block', 'bloco'],\n",
       " ['blocks', 'blocos'],\n",
       " ['bloquado', 'bloqueio'],\n",
       " ['bloque', 'clique'],\n",
       " ['bloqueada', 'bloquear'],\n",
       " ['bloqueado', 'bloqueio'],\n",
       " ['bloqueda', 'bloqueia'],\n",
       " ['bloquedo', 'bloqueio'],\n",
       " ['bloqueios', 'bloqueio'],\n",
       " ['bloqueiou', 'bloqueio'],\n",
       " ['bloqueou', 'bloqueio'],\n",
       " ['bloquiar', 'bloquear'],\n",
       " ['bloquieio', 'bloqueio'],\n",
       " ['bls', 'als'],\n",
       " ['blue', 'que'],\n",
       " ['board', 'boa'],\n",
       " ['boavida', 'movida'],\n",
       " ['bogies', 'botoes'],\n",
       " ['bomda', 'bomba'],\n",
       " ['boracha', 'borracha'],\n",
       " ['borges', 'jorge'],\n",
       " ['borme', 'bom'],\n",
       " ['bornes', 'cortes'],\n",
       " ['borrachas', 'borracha'],\n",
       " ['borrracha', 'borracha'],\n",
       " ['bosch', 'rosca'],\n",
       " ['botoneiras', 'botoneira'],\n",
       " ['botuneira', 'botoneira'],\n",
       " ['boxes', 'gomes'],\n",
       " ['bp', 'tp'],\n",
       " ['bpor', 'por'],\n",
       " ['br', 'sr'],\n",
       " ['bracadeiras', 'bracadeira'],\n",
       " ['brazao', 'razao'],\n",
       " ['brgds', 'bride'],\n",
       " ['bridg', 'bridge'],\n",
       " ['bridgin', 'bridge'],\n",
       " ['brig', 'ria'],\n",
       " ['brin', 'ria'],\n",
       " ['brindg', 'bridge'],\n",
       " ['bsm', 'bom'],\n",
       " ['bucins', 'buzina'],\n",
       " ['bug', 'bus'],\n",
       " ['bumber', 'bumper'],\n",
       " ['bumpe', 'bumper'],\n",
       " ['bumperes', 'bumper'],\n",
       " ['bumpers', 'bumper'],\n",
       " ['bumpoer', 'bumper'],\n",
       " ['bumprer', 'bumper'],\n",
       " ['bunper', 'bumper'],\n",
       " ['buper', 'bumper'],\n",
       " ['burner', 'bumper'],\n",
       " ['busgate', 'resgate'],\n",
       " ['but', 'aut'],\n",
       " ['butoneira', 'botoneira'],\n",
       " ['buzinas', 'buzina'],\n",
       " ['by-pass', 'bypass'],\n",
       " ['byp', 'by'],\n",
       " ['bypasse', 'bypass'],\n",
       " ['cabera', 'camera'],\n",
       " ['cabinas', 'cabina'],\n",
       " ['cabinete', 'gabinete'],\n",
       " ['cablagem', 'colagem'],\n",
       " ['cacostagem', 'acostagem'],\n",
       " ['cacostamento', 'acostamento'],\n",
       " ['caetano', 'cartao'],\n",
       " ['caiam', 'caia'],\n",
       " ['caiem', 'caem'],\n",
       " ['cairam', 'sairam'],\n",
       " ['caleiras', 'caleira'],\n",
       " ['calibracao', 'celebracao'],\n",
       " ['calisto', 'colisao'],\n",
       " ['camarate', 'camara'],\n",
       " ['camopia', 'canopia'],\n",
       " ['canceiro', 'canteiro'],\n",
       " ['cancelada', 'cancelado'],\n",
       " ['cancelados', 'cancelado'],\n",
       " ['cancelou', 'cancelar'],\n",
       " ['cannopia', 'canopia'],\n",
       " ['cannopy', 'canopy'],\n",
       " ['cannot', 'canto'],\n",
       " ['canoly', 'canopy'],\n",
       " ['canopias', 'canopia'],\n",
       " ['canoply', 'canopy'],\n",
       " ['cantinho', 'caminho'],\n",
       " ['cap', 'can'],\n",
       " ['caparica', 'apareca'],\n",
       " ['caq', 'can'],\n",
       " ['car', 'ar'],\n",
       " ['carapeto', 'carreto'],\n",
       " ['cardoso', 'carlos'],\n",
       " ['carecem', 'carece'],\n",
       " ['carla', 'carga'],\n",
       " ['carlosll', 'carlos'],\n",
       " ['carmo', 'caro'],\n",
       " ['caros', 'carlos'],\n",
       " ['carote', 'parte'],\n",
       " ['carregando', 'carregado'],\n",
       " ['carrelo', 'carreto'],\n",
       " ['carrier', 'correr'],\n",
       " ['carrilho', 'carrinho'],\n",
       " ['carrinha', 'carrinho'],\n",
       " ['carrinhos', 'carrinho'],\n",
       " ['casquilhos', 'casquilho'],\n",
       " ['cat', 'cmt'],\n",
       " ['catering', 'catarina'],\n",
       " ['causarem', 'causar'],\n",
       " ['causava', 'causada'],\n",
       " ['cca', 'cc'],\n",
       " ['cclis', 'lis'],\n",
       " ['ccomforme', 'conforme'],\n",
       " ['cconsola', 'consola'],\n",
       " ['cctv', 'ctc'],\n",
       " ['ccu', 'cc'],\n",
       " ['cdg', 'de'],\n",
       " ['cdi', 'ci'],\n",
       " ['cdmelo', 'melo'],\n",
       " ['cecuada', 'recuada'],\n",
       " ['cedam', 'cada'],\n",
       " ['cedex', 'cede'],\n",
       " ['cedi', 'cede'],\n",
       " ['cedida', 'cedido'],\n",
       " ['cee', 'cge'],\n",
       " ['cegada', 'chegada'],\n",
       " ['ceitacao', 'aceitacao'],\n",
       " ['celia', 'cela'],\n",
       " ['centinetros', 'centimetros'],\n",
       " ['cercade', 'cerca'],\n",
       " ['certificados', 'certificado'],\n",
       " ['cesse', 'esse'],\n",
       " ['cette', 'aceite'],\n",
       " ['chaegada', 'chegada'],\n",
       " ['chain', 'chao'],\n",
       " ['chamando', 'chamado'],\n",
       " ['chamarem', 'chamar'],\n",
       " ['change', 'chave'],\n",
       " ['check', 'cheia'],\n",
       " ['chefar', 'chegar'],\n",
       " ['chegadado', 'chegada'],\n",
       " ['chegados', 'chegamos'],\n",
       " ['chegarmos', 'chegamos'],\n",
       " ['chegassem', 'chegarem'],\n",
       " ['chegda', 'chegada'],\n",
       " ['chergada', 'chegada'],\n",
       " ['chock', 'stock'],\n",
       " ['chose', 'shoe'],\n",
       " ['chutes', 'cortes'],\n",
       " ['cifao', 'sifao'],\n",
       " ['circuto', 'circuito'],\n",
       " ['civ', 'ci'],\n",
       " ['ck-in', 'cabin'],\n",
       " ['ckin', 'in'],\n",
       " ['claudia', 'cauda'],\n",
       " ['cles', 'eles'],\n",
       " ['cli', 'ci'],\n",
       " ['clock', 'bloco'],\n",
       " ['clocks', 'blocos'],\n",
       " ['close', 'jose'],\n",
       " ['cmara', 'camara'],\n",
       " ['cmf', 'cmt'],\n",
       " ['cml', 'cmt'],\n",
       " ['cmpletamente', 'completamente'],\n",
       " ['cmps', 'mas'],\n",
       " ['cmptos', 'cpts'],\n",
       " ['cmpts', 'cpts'],\n",
       " ['cms', 'cmt'],\n",
       " ['cnopy', 'canopy'],\n",
       " ['cnunes', 'nunes'],\n",
       " ['coclocada', 'colocada'],\n",
       " ['cod', 'com'],\n",
       " ['codico', 'codigo'],\n",
       " ['codificacao', 'modificacao'],\n",
       " ['coerentes', 'corrente'],\n",
       " ['cofragem', 'colagem'],\n",
       " ['cofres', 'cofre'],\n",
       " ['colaboaradora', 'colaborador'],\n",
       " ['colaboradora', 'colaborador'],\n",
       " ['colaboraram', 'colaboracao'],\n",
       " ['colacacao', 'colocacao'],\n",
       " ['coloboracao', 'colaboracao'],\n",
       " ['coloc', 'coloca'],\n",
       " ['coloca-la', 'colocada'],\n",
       " ['coloca-lo', 'colocado'],\n",
       " ['colocadao', 'colocada'],\n",
       " ['colocamos', 'colocados'],\n",
       " ['colocara', 'colocada'],\n",
       " ['colocaram', 'colocada'],\n",
       " ['colocarmos', 'colocados'],\n",
       " ['colocda', 'colocada'],\n",
       " ['comecei', 'comecou'],\n",
       " ['comforme', 'conforme'],\n",
       " ['comment', 'content'],\n",
       " ['compactador', 'computador'],\n",
       " ['compactivel', 'compativel'],\n",
       " ['companies', 'companhias'],\n",
       " ['company', 'compact'],\n",
       " ['comparadas', 'comparacao'],\n",
       " ['compareceram', 'comparecer'],\n",
       " ['compativeis', 'compativel'],\n",
       " ['compencou', 'comecou'],\n",
       " ['completada', 'completar'],\n",
       " ['completado', 'completar'],\n",
       " ['completarem', 'completar'],\n",
       " ['completava', 'completar'],\n",
       " ['completemente', 'completamente'],\n",
       " ['completou', 'completo'],\n",
       " ['compreencao', 'compreensao'],\n",
       " ['comprovado', 'comprovar'],\n",
       " ['comsegue', 'consegue'],\n",
       " ['comsiderado', 'considerado'],\n",
       " ['comunacacao', 'comunicacao'],\n",
       " ['comunicada', 'comunicado'],\n",
       " ['comunicados', 'comunicado'],\n",
       " ['comunicamos', 'comunicado'],\n",
       " ['comunicarem', 'comunicar'],\n",
       " ['comutado', 'comecado'],\n",
       " ['comutador', 'computador'],\n",
       " ['comutadores', 'contadores'],\n",
       " ['comutados', 'computador'],\n",
       " ['comutando', 'comando'],\n",
       " ['comutava', 'comecava'],\n",
       " ['concatcto', 'contacto'],\n",
       " ['conceicao', 'condicao'],\n",
       " ['concluida', 'concluido'],\n",
       " ['concluidos', 'concluido'],\n",
       " ['concluindo', 'concluido'],\n",
       " ['concluirem', 'concluir'],\n",
       " ['cond', 'cone'],\n",
       " ['condencados', 'condensados'],\n",
       " ['conderado', 'considerado'],\n",
       " ['condicinado', 'condicionado'],\n",
       " ['condicionada', 'condicionado'],\n",
       " ['condioconado', 'condicionado'],\n",
       " ['conduzindo', 'conduzido'],\n",
       " ['conect', 'connect'],\n",
       " ['conecta', 'correcta'],\n",
       " ['conectada', 'comecada'],\n",
       " ['conectado', 'comecado'],\n",
       " ['conectores', 'condutores'],\n",
       " ['conetado', 'contador'],\n",
       " ['coneted', 'contem'],\n",
       " ['confidenciais', 'confidencial'],\n",
       " ['confidential', 'confidencial'],\n",
       " ['configura', 'configurar'],\n",
       " ['configurada', 'configuracao'],\n",
       " ['configurado', 'configuracao'],\n",
       " ['configuravam', 'configuracao'],\n",
       " ['configurou', 'configurar'],\n",
       " ['confirmamos', 'confirmado'],\n",
       " ['confirmaram', 'confirmacao'],\n",
       " ['confirme', 'conforme'],\n",
       " ['confirmei', 'conforme'],\n",
       " ['confirmou', 'confirmo'],\n",
       " ['conluir', 'concluir'],\n",
       " ['conmpareceu', 'compareceu'],\n",
       " ['conmstrangimento', 'constrangimento'],\n",
       " ['connected', 'connect'],\n",
       " ['conpensacao', 'condensacao'],\n",
       " ['cons', 'bons'],\n",
       " ['conseg', 'consegue'],\n",
       " ['consegiui', 'consegui'],\n",
       " ['conseguiam', 'conseguia'],\n",
       " ['conseguimenos', 'conseguimos'],\n",
       " ['conseguio', 'conseguiu'],\n",
       " ['conseguiria', 'conseguia'],\n",
       " ['conseguisse', 'consegue-se'],\n",
       " ['conseguiu-se', 'consegue-se'],\n",
       " ['consent', 'content'],\n",
       " ['consequently', 'consequente'],\n",
       " ['consguiu', 'conseguiu'],\n",
       " ['consid', 'consta'],\n",
       " ['consider', 'considera'],\n",
       " ['consideramos', 'considerados'],\n",
       " ['considerou-se', 'considera-se'],\n",
       " ['consolas', 'consola'],\n",
       " ['constactou', 'contacto'],\n",
       " ['constatado', 'constatar'],\n",
       " ['constataram', 'constatar'],\n",
       " ['constatei', 'constante'],\n",
       " ['constatou', 'constatar'],\n",
       " ['constrangmento', 'constrangimento'],\n",
       " ['consultadas', 'consultada'],\n",
       " ['consumos', 'consumo'],\n",
       " ['cont', 'conta'],\n",
       " ['contabiliza', 'contabilizar'],\n",
       " ['contabilizado', 'contabilizar'],\n",
       " ['contacato', 'contacto'],\n",
       " ['contact', 'contacto'],\n",
       " ['contacta', 'contacto'],\n",
       " ['contactada', 'contratada'],\n",
       " ['contactado', 'contacto'],\n",
       " ['contactados', 'contactos'],\n",
       " ['contactam', 'contacto'],\n",
       " ['contactar', 'contacto'],\n",
       " ['contactato', 'contacto'],\n",
       " ['contacte', 'contacto'],\n",
       " ['contactor', 'contacto'],\n",
       " ['contactou', 'contacto'],\n",
       " ['contagens', 'contagem'],\n",
       " ['contain', 'conta'],\n",
       " ['contatado', 'contratada'],\n",
       " ['contato', 'contacto'],\n",
       " ['contator', 'contador'],\n",
       " ['contento', 'content'],\n",
       " ['contents', 'content'],\n",
       " ['contenus', 'continua'],\n",
       " ['continou', 'continua'],\n",
       " ['continuando', 'continuado'],\n",
       " ['continuava', 'continuavam'],\n",
       " ['continuou', 'continua'],\n",
       " ['contract', 'contacto'],\n",
       " ['contracts', 'contacto'],\n",
       " ['contreras', 'contrarias'],\n",
       " ['control', 'controlo'],\n",
       " ['controladores', 'controlador'],\n",
       " ['controlodor', 'controlador'],\n",
       " ['coord', 'cor'],\n",
       " ['coordenador', 'coordenado'],\n",
       " ['coordernadas', 'coordenadas'],\n",
       " ['coorrdenadas', 'coordenadas'],\n",
       " ['copiada', 'copia'],\n",
       " ['copied', 'copia'],\n",
       " ['copier', 'copia'],\n",
       " ['copio', 'copia'],\n",
       " ['coplagem', 'colagem'],\n",
       " ['coprrigida', 'corrigida'],\n",
       " ['copy', 'copa'],\n",
       " ['corformidade', 'conformidade'],\n",
       " ['correa', 'correu'],\n",
       " ['correcao', 'correccao'],\n",
       " ['correcoes', 'correccoes'],\n",
       " ['correct', 'correcta'],\n",
       " ['corregida', 'corrigida'],\n",
       " ['corregir', 'corrigir'],\n",
       " ['corremos', 'corredor'],\n",
       " ['correponde', 'corresponde'],\n",
       " ['correpondente', 'correspondente'],\n",
       " ['correram', 'decorreram'],\n",
       " ['corresponderam', 'corresponder'],\n",
       " ['correta', 'correcta'],\n",
       " ['corretamente', 'correctamente'],\n",
       " ['correto', 'correcto'],\n",
       " ['corretos', 'correctos'],\n",
       " ['corrgida', 'corrigida'],\n",
       " ['corrifgida', 'corrigida'],\n",
       " ['corrifgir', 'corrigir'],\n",
       " ['corriga', 'corria'],\n",
       " ['corrigada', 'corrigida'],\n",
       " ['corrigda', 'corrigida'],\n",
       " ['corrigidaa', 'corrigida'],\n",
       " ['corrigidas', 'corrigida'],\n",
       " ['corrigidoo', 'corrigido'],\n",
       " ['corrigidos', 'corrigido'],\n",
       " ['corrigira', 'corrigida'],\n",
       " ['corrigiu', 'corrigir'],\n",
       " ['corriguir', 'corrigir'],\n",
       " ['corriida', 'corrigida'],\n",
       " ['corriido', 'corrigido'],\n",
       " ['corrijam', 'corrimao'],\n",
       " ['corrimaos', 'corrimao'],\n",
       " ['corrompida', 'corrompido'],\n",
       " ['corrrigida', 'corrigida'],\n",
       " ['cosa', 'rosa'],\n",
       " ['costagem', 'acostagem'],\n",
       " ['cou', 'com'],\n",
       " ['could', 'cola'],\n",
       " ['counter', 'conter'],\n",
       " ['course', 'curso'],\n",
       " ['cp', 'tp'],\n",
       " ['cpmts', 'cpts'],\n",
       " ['cps', 'gps'],\n",
       " ['cptos', 'cpts'],\n",
       " ['cpu', 'com'],\n",
       " ['criava', 'criada'],\n",
       " ['csp', 'com'],\n",
       " ['ct', 'ctc'],\n",
       " ['ctt', 'ctc'],\n",
       " ['cube', 'cuba'],\n",
       " ['cump', 'com'],\n",
       " ['cumptos', 'custos'],\n",
       " ['cumpts', 'cpts'],\n",
       " ['cumts', 'cmt'],\n",
       " ['cur', 'cor'],\n",
       " ['cursoda', 'curso'],\n",
       " ['cursso', 'curso'],\n",
       " ['cusro', 'custo'],\n",
       " ['cv', 'cc'],\n",
       " ['cvdp', 'cada'],\n",
       " ['cvs', 'vs'],\n",
       " ['cx', 'cc'],\n",
       " ['cz', 'cc'],\n",
       " ['dadaa', 'dada'],\n",
       " ['dadaos', 'dadas'],\n",
       " ['dafety', 'safety'],\n",
       " ['dager', 'fazer'],\n",
       " ['dals', 'das'],\n",
       " ['danger', 'ranger'],\n",
       " ['daniel', 'manuel'],\n",
       " ['danificados', 'danificado'],\n",
       " ['danificando', 'danificado'],\n",
       " ['danificava', 'danificada'],\n",
       " ['danificou', 'danificar'],\n",
       " ['dans', 'das'],\n",
       " ['daproteccao', 'proteccao'],\n",
       " ['darei', 'darem'],\n",
       " ['date', 'ate'],\n",
       " ['dc', 'de'],\n",
       " ['dd', 'de'],\n",
       " ['dda', 'da'],\n",
       " ['ddi', 'dai'],\n",
       " ['dea', 'de'],\n",
       " ['dealarmes', 'alarmes'],\n",
       " ['debitem', 'debitar'],\n",
       " ['dec', 'de'],\n",
       " ['decorrwer', 'decorrer'],\n",
       " ['deemergencia', 'emergencia'],\n",
       " ['deetalhes', 'detalhes'],\n",
       " ['deevido', 'devido'],\n",
       " ['defeciente', 'deficiente'],\n",
       " ['defenida', 'definida'],\n",
       " ['defenido', 'definido'],\n",
       " ['defenitiva', 'definitiva'],\n",
       " ['deferencial', 'diferencial'],\n",
       " ['deficente', 'deficiente'],\n",
       " ['deficultou', 'dificultou'],\n",
       " ['defim', 'fim'],\n",
       " ['defucientes', 'deficientes'],\n",
       " ['degradadas', 'degradacao'],\n",
       " ['degradado', 'degradacao'],\n",
       " ['dei-lhe', 'detalhe'],\n",
       " ['deixa-la', 'deixada'],\n",
       " ['deixando-as', 'deixando-a'],\n",
       " ['deixava', 'deixada'],\n",
       " ['deixo', 'deixou'],\n",
       " ['deixou-a', 'deixou'],\n",
       " ['del', 'de'],\n",
       " ['delay', 'delas'],\n",
       " ['deleay', 'deles'],\n",
       " ['delete', 'deste'],\n",
       " ['demasiiado', 'demasiado'],\n",
       " ['demonstrava', 'demonstra'],\n",
       " ['demoramos', 'demorado'],\n",
       " ['demorava', 'demorado'],\n",
       " ['demorei', 'demorou'],\n",
       " ['depoi', 'depois'],\n",
       " ['depoios', 'depois'],\n",
       " ['depresa', 'depressa'],\n",
       " ['der', 'de'],\n",
       " ['derca', 'cerca'],\n",
       " ['deriavado', 'derivado'],\n",
       " ['derrame', 'deram'],\n",
       " ['dersembarque', 'desembarque'],\n",
       " ['desactivada', 'desactivar'],\n",
       " ['desactivado', 'desactivar'],\n",
       " ['desactivo', 'desactivar'],\n",
       " ['desactuar', 'desactivar'],\n",
       " ['desalinhada', 'desalinhado'],\n",
       " ['desaparecem', 'desapareceu'],\n",
       " ['desapertada', 'desapertar'],\n",
       " ['desapertado', 'desapertar'],\n",
       " ['desaperto', 'desapertar'],\n",
       " ['desbloquea', 'desbloquear'],\n",
       " ['desbloqueada', 'desbloquear'],\n",
       " ['desbloqueado', 'desbloquear'],\n",
       " ['desbloqueador', 'desbloquear'],\n",
       " ['desbloqueda', 'desbloquear'],\n",
       " ['desbloqueio', 'desbloquear'],\n",
       " ['desbloqueou', 'desbloquear'],\n",
       " ['desc', 'desce'],\n",
       " ['descaida', 'descida'],\n",
       " ['descartam', 'descartar'],\n",
       " ['descend', 'descendo'],\n",
       " ['descended', 'descendo'],\n",
       " ['descent', 'desce'],\n",
       " ['descido', 'descida'],\n",
       " ['descobria', 'descobrir'],\n",
       " ['desconheco', 'desconhece'],\n",
       " ['descritos', 'descrito'],\n",
       " ['desculpe', 'desculpa'],\n",
       " ['desembarcado', 'desembarcar'],\n",
       " ['desembarcaram', 'desembarcar'],\n",
       " ['desembarqcar', 'desembarcar'],\n",
       " ['desembarue', 'desembarque'],\n",
       " ['desemkbarque', 'desembarque'],\n",
       " ['desencaixar', 'desencadear'],\n",
       " ['desencosta', 'desencostar'],\n",
       " ['desencostada', 'desencostar'],\n",
       " ['desencostado', 'desencostar'],\n",
       " ['desencostava', 'desencostar'],\n",
       " ['desencosto', 'desencostar'],\n",
       " ['desencostou', 'desencostar'],\n",
       " ['desenrrola', 'desenrolar'],\n",
       " ['desenrrolar', 'desenrolar'],\n",
       " ...]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_switcheroo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
